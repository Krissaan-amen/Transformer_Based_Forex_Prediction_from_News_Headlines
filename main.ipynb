{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AlbertTokenizer, AlbertModel, AlbertForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Function\n",
    "def plot_metrics(train_losses, val_losses, val_accuracies):\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  # Sets the seed for all devices, including MPS\n",
    "    if torch.cuda.is_available():  # For CUDA if present\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Disable deterministic behavior for MPS (optional, as it's not strictly needed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model Class\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        # self.base_model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)  \n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # Get the original outputs from the base model\n",
    "        output = self.base_model(**kwargs)\n",
    "\n",
    "        # Apply dropout to logits\n",
    "        logits = self.dropout(output.logits)\n",
    "\n",
    "        # Return a SequenceClassifierOutput with modified logits\n",
    "        return SequenceClassifierOutput(\n",
    "            logits=logits,\n",
    "            hidden_states=output.hidden_states,\n",
    "            attentions=output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "def tokenize_data(data, tokenizer, max_length=MAX_LEN):\n",
    "    return tokenizer(\n",
    "        data[\"title\"].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    total_loss = 0  # Track validation loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, axis=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    return predictions, true_labels, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, epochs=EPOCHS):\n",
    "    best_accuracy = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    \n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase (Use val_loader instead of test_loader)\n",
    "        predictions, true_labels, avg_val_loss = evaluate_model(model, val_loader, device, loss_fn)\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 256\n",
    "WEIGHT_DECAY = 0.0005\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "df = pd.read_csv(\"merged_data/eurusd_daily_news.csv\")\n",
    "# df = pd.read_csv(\"merged_data/eurusd_15min_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers and Models\n",
    "# model_name = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "# model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# model_name = \"albert/albert-base-v2\"\n",
    "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = AlbertTokenizer.from_pretrained('albert/albert-base-v2')\n",
    "\n",
    "model = CustomModel(model_name, num_labels=2)\n",
    "df_cropped = df[[\"title\", \"movement\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and Test\n",
    "train_df, temp_df = train_test_split(df_cropped, test_size=0.3, stratify=df_cropped[\"movement\"], random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"movement\"], random_state=SEED)\n",
    "\n",
    "# Tokenize Data\n",
    "train_encodings = tokenize_data(train_df, tokenizer)\n",
    "val_encodings = tokenize_data(val_df, tokenizer)\n",
    "test_encodings = tokenize_data(test_df, tokenizer)\n",
    "\n",
    "# Convert Labels to Tensors\n",
    "train_labels = torch.tensor(train_df[\"movement\"].values)\n",
    "val_labels = torch.tensor(val_df[\"movement\"].values)\n",
    "test_labels = torch.tensor(test_df[\"movement\"].values)\n",
    "\n",
    "# Create Dataset Objects\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_df[\"movement\"]),\n",
    "    y=train_df[\"movement\"]\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Training Setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"MPS Available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    " \n",
    "# Train the Model\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "print(\"Fine-Tuning the Model...\")\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, optimizer, device\n",
    ")\n",
    "# Plot Metrics\n",
    "plot_metrics(train_losses, val_losses, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Fine-Tuned Model\n",
    "# Load the best model (selected based on validation accuracy)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Evaluate on Test Set (Final Evaluation)\n",
    "print(\"Evaluating Fine-Tuned Model on Test Set...\")\n",
    "test_predictions, test_true_labels, _ = evaluate_model(model, test_loader, device, torch.nn.CrossEntropyLoss())\n",
    "test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print Full Classification Report\n",
    "print(classification_report(test_true_labels, test_predictions, target_names=['down', 'up']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
